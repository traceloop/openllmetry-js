{
  "log": {
    "_recordingName": "Test Langchain instrumentation/should set attributes in span for tools instrumentation",
    "creator": {
      "comment": "persister:fs",
      "name": "Polly.JS",
      "version": "6.0.6"
    },
    "entries": [
      {
        "_id": "926cd215cc39afa1cae6237de5d30ce8",
        "_order": 0,
        "cache": {},
        "request": {
          "bodySize": 0,
          "cookies": [],
          "headers": [],
          "headersSize": 109,
          "httpVersion": "HTTP/1.1",
          "method": "GET",
          "queryString": [
            {
              "name": "action",
              "value": "query"
            },
            {
              "name": "list",
              "value": "search"
            },
            {
              "name": "srsearch",
              "value": "Langchain"
            },
            {
              "name": "format",
              "value": "json"
            }
          ],
          "url": "https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=Langchain&format=json"
        },
        "response": {
          "bodySize": 3119,
          "content": {
            "mimeType": "application/json; charset=utf-8",
            "size": 3119,
            "text": "{\"batchcomplete\":\"\",\"query\":{\"searchinfo\":{\"totalhits\":9,\"suggestion\":\"lang chain\",\"suggestionsnippet\":\"lang chain\"},\"search\":[{\"ns\":0,\"title\":\"LangChain\",\"pageid\":73577626,\"size\":20125,\"wordcount\":884,\"snippet\":\"Free and open-source software portal <span class=\\\"searchmatch\\\">LangChain</span> is a software framework that helps facilitate the integration of large language models (LLMs) into applications\",\"timestamp\":\"2025-08-03T10:28:57Z\"},{\"ns\":0,\"title\":\"Vector database\",\"pageid\":74020014,\"size\":24769,\"wordcount\":1700,\"snippet\":\"at master \\u00b7 weaviate/weaviate&quot;. GitHub. Retrieved 2023-10-29. &quot;<span class=\\\"searchmatch\\\">Langchain</span> YDB&quot;. <span class=\\\"searchmatch\\\">Langchain</span>. Retrieved 2025-07-26. &quot;Vector Search&quot;. Retrieved 2025-07-26.\",\"timestamp\":\"2025-08-10T17:47:49Z\"},{\"ns\":0,\"title\":\"Retrieval-augmented generation\",\"pageid\":75229858,\"size\":23832,\"wordcount\":2477,\"snippet\":\"considerations should be taken for pdf files. Libraries such as Unstructured or <span class=\\\"searchmatch\\\">Langchain</span> can assist with this method. Rather than using documents as a source to\",\"timestamp\":\"2025-08-13T08:07:20Z\"},{\"ns\":0,\"title\":\"Model Context Protocol\",\"pageid\":79706999,\"size\":20650,\"wordcount\":1856,\"snippet\":\"computers or programsPages displaying short descriptions of redirect targets <span class=\\\"searchmatch\\\">LangChain</span>\\u00a0\\u2013 Language model application development framework Machine learning\\u00a0\\u2013\",\"timestamp\":\"2025-08-08T00:04:53Z\"},{\"ns\":0,\"title\":\"Intelligent agent\",\"pageid\":2711317,\"size\":73980,\"wordcount\":6899,\"snippet\":\"Rumination, and Coze (by ByteDance). Frameworks for building AI agents include <span class=\\\"searchmatch\\\">LangChain</span>, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\",\"timestamp\":\"2025-08-05T00:21:47Z\"},{\"ns\":0,\"title\":\"Milvus (vector database)\",\"pageid\":77949393,\"size\":12688,\"wordcount\":1005,\"snippet\":\"monitoring and alerts, as well as generative AI frameworks Haystack, <span class=\\\"searchmatch\\\">LangChain</span>, IBM Watsonx, and those provided by OpenAI. Several storage providers\",\"timestamp\":\"2025-08-14T17:45:48Z\"},{\"ns\":0,\"title\":\"FAISS\",\"pageid\":78493979,\"size\":13889,\"wordcount\":1163,\"snippet\":\"similarity search benchmarks. FAISS has an integration with Haystack, <span class=\\\"searchmatch\\\">LangChain</span> frameworks. Various advanced code snippets for FAISS can be found on its\",\"timestamp\":\"2025-08-14T17:12:52Z\"},{\"ns\":0,\"title\":\"DataStax\",\"pageid\":37892981,\"size\":22109,\"wordcount\":1861,\"snippet\":\"commercial offering for RAG (retrieval-augmented generation) based on <span class=\\\"searchmatch\\\">LangChain</span> and Astra DB vector search. On February 25, 2025, IBM announced its intention\",\"timestamp\":\"2025-08-12T12:49:52Z\"},{\"ns\":0,\"title\":\"Sentence embedding\",\"pageid\":58348103,\"size\":9223,\"wordcount\":973,\"snippet\":\"knowledge bases through the usage of vector indexing for semantic search. <span class=\\\"searchmatch\\\">LangChain</span> for instance utilizes sentence transformers for purposes of indexing documents\",\"timestamp\":\"2025-01-10T19:07:44Z\"}]}}"
          },
          "cookies": [
            {
              "domain": ".wikipedia.org",
              "expires": "2026-08-17T00:00:00.000Z",
              "httpOnly": true,
              "name": "WMF-Uniq",
              "path": "/",
              "sameSite": "None",
              "secure": true,
              "value": "E0SwSg5XH7cqc0y5i2fmcAJSAAAAAFvd9gVuCw9-H3SUHwh-I25gDlOVDWFKd9pK"
            }
          ],
          "headers": [
            {
              "name": "age",
              "value": "0"
            },
            {
              "name": "cache-control",
              "value": "private, must-revalidate, max-age=0"
            },
            {
              "name": "content-disposition",
              "value": "inline; filename=api-result.json"
            },
            {
              "name": "content-encoding",
              "value": "gzip"
            },
            {
              "name": "content-type",
              "value": "application/json; charset=utf-8"
            },
            {
              "name": "date",
              "value": "Sun, 17 Aug 2025 13:58:25 GMT"
            },
            {
              "name": "nel",
              "value": "{ \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}"
            },
            {
              "name": "report-to",
              "value": "{ \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }"
            },
            {
              "name": "server",
              "value": "mw-api-ext.eqiad.main-8769686bf-bphvg"
            },
            {
              "name": "server-timing",
              "value": "cache;desc=\"pass\", host;desc=\"cp3067\""
            },
            {
              "name": "set-cookie",
              "value": "WMF-Uniq=E0SwSg5XH7cqc0y5i2fmcAJSAAAAAFvd9gVuCw9-H3SUHwh-I25gDlOVDWFKd9pK;Domain=.wikipedia.org;Path=/;HttpOnly;secure;SameSite=None;Expires=Mon, 17 Aug 2026 00:00:00 GMT"
            },
            {
              "name": "strict-transport-security",
              "value": "max-age=106384710; includeSubDomains; preload"
            },
            {
              "name": "transfer-encoding",
              "value": "chunked"
            },
            {
              "name": "vary",
              "value": "Accept-Encoding,X-Subdomain,Treat-as-Untrusted,X-Forwarded-Proto,Cookie,Authorization"
            },
            {
              "name": "x-analytics",
              "value": ""
            },
            {
              "name": "x-cache",
              "value": "cp3067 miss, cp3067 pass"
            },
            {
              "name": "x-cache-status",
              "value": "pass"
            },
            {
              "name": "x-client-ip",
              "value": "31.154.188.106"
            },
            {
              "name": "x-content-type-options",
              "value": "nosniff"
            },
            {
              "name": "x-frame-options",
              "value": "DENY"
            },
            {
              "name": "x-search-id",
              "value": "ccfh2bbkdz3kq0ked7yuv6lda"
            }
          ],
          "headersSize": 1219,
          "httpVersion": "HTTP/1.1",
          "redirectURL": "",
          "status": 200,
          "statusText": "OK"
        },
        "startedDateTime": "2025-08-17T13:58:25.412Z",
        "time": 474,
        "timings": {
          "blocked": -1,
          "connect": -1,
          "dns": -1,
          "receive": 0,
          "send": 0,
          "ssl": -1,
          "wait": 474
        }
      },
      {
        "_id": "926cd215cc39afa1cae6237de5d30ce8",
        "_order": 1,
        "cache": {},
        "request": {
          "bodySize": 0,
          "cookies": [],
          "headers": [],
          "headersSize": 138,
          "httpVersion": "HTTP/1.1",
          "method": "GET",
          "queryString": [
            {
              "name": "action",
              "value": "query"
            },
            {
              "name": "prop",
              "value": "extracts"
            },
            {
              "name": "explaintext",
              "value": "true"
            },
            {
              "name": "redirects",
              "value": "1"
            },
            {
              "name": "format",
              "value": "json"
            },
            {
              "name": "titles",
              "value": "LangChain"
            }
          ],
          "url": "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&explaintext=true&redirects=1&format=json&titles=LangChain"
        },
        "response": {
          "bodySize": 3613,
          "content": {
            "mimeType": "application/json; charset=utf-8",
            "size": 3613,
            "text": "{\"batchcomplete\":\"\",\"query\":{\"pages\":{\"73577626\":{\"pageid\":73577626,\"ns\":0,\"title\":\"LangChain\",\"extract\":\"LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n== History ==\\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project's Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.\\nIn the third quarter of 2023, the LangChain Expression Language (LCEL) was introduced, which provides a declarative way to define chains of actions.\\nIn October 2023 LangChain introduced LangServe, a deployment tool to host LCEL code as a production-ready API.\\nIn February 2024 LangChain released LangSmith, a closed-source observability and evaluation platform for LLM applications, and announced a US $25 million Series A led by Sequoia Capital. On 14 May 2025 the company launched LangGraph Platform into general availability, providing managed infrastructure for deploying long-running, stateful AI agents; the beta had already been used by nearly 400 companies.\\n\\n\\n== Capabilities ==\\nLangChain's developers highlight the framework's applicability to use-cases including chatbots, retrieval-augmented generation,  document summarization, and synthetic data generation.\\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \\\"todo\\\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database to store and retrieve vector embeddings; Weaviate vector database to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\\n\\n\\n== LangChain tools ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nOfficial website\\nDiscord server support hub\\nLangchain-ai on GitHub\"}}}}"
          },
          "cookies": [
            {
              "domain": ".wikipedia.org",
              "expires": "2026-08-17T00:00:00.000Z",
              "httpOnly": true,
              "name": "WMF-Uniq",
              "path": "/",
              "sameSite": "None",
              "secure": true,
              "value": "RXfqD_aboqiz_BoNpFBgpAJSAAAAAFvdZyPmY9dQxyUEzi8X539CB9cL_ECsHSRP"
            }
          ],
          "headers": [
            {
              "name": "age",
              "value": "0"
            },
            {
              "name": "cache-control",
              "value": "private, must-revalidate, max-age=0"
            },
            {
              "name": "content-disposition",
              "value": "inline; filename=api-result.json"
            },
            {
              "name": "content-encoding",
              "value": "gzip"
            },
            {
              "name": "content-type",
              "value": "application/json; charset=utf-8"
            },
            {
              "name": "date",
              "value": "Sun, 17 Aug 2025 13:58:26 GMT"
            },
            {
              "name": "nel",
              "value": "{ \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}"
            },
            {
              "name": "report-to",
              "value": "{ \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }"
            },
            {
              "name": "server",
              "value": "mw-api-ext.eqiad.main-8769686bf-4z859"
            },
            {
              "name": "server-timing",
              "value": "cache;desc=\"pass\", host;desc=\"cp3067\""
            },
            {
              "name": "set-cookie",
              "value": "WMF-Uniq=RXfqD_aboqiz_BoNpFBgpAJSAAAAAFvdZyPmY9dQxyUEzi8X539CB9cL_ECsHSRP;Domain=.wikipedia.org;Path=/;HttpOnly;secure;SameSite=None;Expires=Mon, 17 Aug 2026 00:00:00 GMT"
            },
            {
              "name": "strict-transport-security",
              "value": "max-age=106384710; includeSubDomains; preload"
            },
            {
              "name": "transfer-encoding",
              "value": "chunked"
            },
            {
              "name": "vary",
              "value": "Accept-Encoding,X-Subdomain,Treat-as-Untrusted,X-Forwarded-Proto,Cookie,Authorization"
            },
            {
              "name": "x-analytics",
              "value": ""
            },
            {
              "name": "x-cache",
              "value": "cp3067 miss, cp3067 pass"
            },
            {
              "name": "x-cache-status",
              "value": "pass"
            },
            {
              "name": "x-client-ip",
              "value": "31.154.188.106"
            },
            {
              "name": "x-content-type-options",
              "value": "nosniff"
            },
            {
              "name": "x-frame-options",
              "value": "DENY"
            }
          ],
          "headersSize": 1179,
          "httpVersion": "HTTP/1.1",
          "redirectURL": "",
          "status": 200,
          "statusText": "OK"
        },
        "startedDateTime": "2025-08-17T13:58:25.889Z",
        "time": 211,
        "timings": {
          "blocked": -1,
          "connect": -1,
          "dns": -1,
          "receive": 0,
          "send": 0,
          "ssl": -1,
          "wait": 211
        }
      },
      {
        "_id": "926cd215cc39afa1cae6237de5d30ce8",
        "_order": 2,
        "cache": {},
        "request": {
          "bodySize": 0,
          "cookies": [],
          "headers": [],
          "headersSize": 146,
          "httpVersion": "HTTP/1.1",
          "method": "GET",
          "queryString": [
            {
              "name": "action",
              "value": "query"
            },
            {
              "name": "prop",
              "value": "extracts"
            },
            {
              "name": "explaintext",
              "value": "true"
            },
            {
              "name": "redirects",
              "value": "1"
            },
            {
              "name": "format",
              "value": "json"
            },
            {
              "name": "titles",
              "value": "Vector database"
            }
          ],
          "url": "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&explaintext=true&redirects=1&format=json&titles=Vector%20database"
        },
        "response": {
          "bodySize": 3383,
          "content": {
            "mimeType": "application/json; charset=utf-8",
            "size": 3383,
            "text": "{\"batchcomplete\":\"\",\"query\":{\"pages\":{\"74020014\":{\"pageid\":74020014,\"ns\":0,\"title\":\"Vector database\",\"extract\":\"A vector database, vector store or vector search engine is a database that uses the vector space model to store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more approximate nearest neighbor algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\\nVectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector's position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\\nVector databases can be used for similarity search, semantic search, multi-modal search, recommendations engines, large language models (LLMs), object detection,  etc.\\nVector databases are also often used to implement retrieval-augmented generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \\\"embedding\\\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.\\n\\n\\n== Techniques ==\\nThe most important techniques for similarity search on high-dimensional vectors include:\\n\\nHierarchical Navigable Small World (HNSW) graphs\\nLocality-sensitive Hashing (LSH) and Sketching\\nProduct Quantization (PQ)\\nInverted Files\\nand combinations of these techniques.\\nIn recent benchmarks, HNSW-based implementations have been among the best performers. Conferences such as the International Conference on Similarity Search and Applications, SISAP and the Conference on Neural Information Processing Systems (NeurIPS) host competitions on vector search in large databases.\\n\\n\\n== Implementations ==\\n\\n\\n== See also ==\\nCurse of dimensionality \\u2013 Difficulties arising when analyzing data with many aspects (\\\"dimensions\\\")\\nMachine learning \\u2013 Study of algorithms that improve automatically through experience\\nNearest neighbor search \\u2013 Optimization problem in computer science\\nRecommender system \\u2013 System to predict users' preferences\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nSawers, Paul (2024-04-20). \\\"Why vector databases are having a moment as the AI hype cycle peaks\\\". TechCrunch. Retrieved 2024-04-23.\"}}}}"
          },
          "cookies": [
            {
              "domain": ".wikipedia.org",
              "expires": "2026-08-17T00:00:00.000Z",
              "httpOnly": true,
              "name": "WMF-Uniq",
              "path": "/",
              "sameSite": "None",
              "secure": true,
              "value": "kWu6Zs6xpI0A3Jm26osvDQJSAAAAAFvd3VJMCr12k1tMQLT499cOHbBXtMGMEYRC"
            }
          ],
          "headers": [
            {
              "name": "age",
              "value": "0"
            },
            {
              "name": "cache-control",
              "value": "private, must-revalidate, max-age=0"
            },
            {
              "name": "content-disposition",
              "value": "inline; filename=api-result.json"
            },
            {
              "name": "content-encoding",
              "value": "gzip"
            },
            {
              "name": "content-type",
              "value": "application/json; charset=utf-8"
            },
            {
              "name": "date",
              "value": "Sun, 17 Aug 2025 13:58:26 GMT"
            },
            {
              "name": "nel",
              "value": "{ \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}"
            },
            {
              "name": "report-to",
              "value": "{ \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }"
            },
            {
              "name": "server",
              "value": "mw-api-ext.eqiad.main-8769686bf-987bc"
            },
            {
              "name": "server-timing",
              "value": "cache;desc=\"pass\", host;desc=\"cp3067\""
            },
            {
              "name": "set-cookie",
              "value": "WMF-Uniq=kWu6Zs6xpI0A3Jm26osvDQJSAAAAAFvd3VJMCr12k1tMQLT499cOHbBXtMGMEYRC;Domain=.wikipedia.org;Path=/;HttpOnly;secure;SameSite=None;Expires=Mon, 17 Aug 2026 00:00:00 GMT"
            },
            {
              "name": "strict-transport-security",
              "value": "max-age=106384710; includeSubDomains; preload"
            },
            {
              "name": "transfer-encoding",
              "value": "chunked"
            },
            {
              "name": "vary",
              "value": "Accept-Encoding,X-Subdomain,Treat-as-Untrusted,X-Forwarded-Proto,Cookie,Authorization"
            },
            {
              "name": "x-analytics",
              "value": ""
            },
            {
              "name": "x-cache",
              "value": "cp3067 miss, cp3067 pass"
            },
            {
              "name": "x-cache-status",
              "value": "pass"
            },
            {
              "name": "x-client-ip",
              "value": "31.154.188.106"
            },
            {
              "name": "x-content-type-options",
              "value": "nosniff"
            },
            {
              "name": "x-frame-options",
              "value": "DENY"
            }
          ],
          "headersSize": 1179,
          "httpVersion": "HTTP/1.1",
          "redirectURL": "",
          "status": 200,
          "statusText": "OK"
        },
        "startedDateTime": "2025-08-17T13:58:26.102Z",
        "time": 199,
        "timings": {
          "blocked": -1,
          "connect": -1,
          "dns": -1,
          "receive": 0,
          "send": 0,
          "ssl": -1,
          "wait": 199
        }
      },
      {
        "_id": "926cd215cc39afa1cae6237de5d30ce8",
        "_order": 3,
        "cache": {},
        "request": {
          "bodySize": 0,
          "cookies": [],
          "headers": [],
          "headersSize": 161,
          "httpVersion": "HTTP/1.1",
          "method": "GET",
          "queryString": [
            {
              "name": "action",
              "value": "query"
            },
            {
              "name": "prop",
              "value": "extracts"
            },
            {
              "name": "explaintext",
              "value": "true"
            },
            {
              "name": "redirects",
              "value": "1"
            },
            {
              "name": "format",
              "value": "json"
            },
            {
              "name": "titles",
              "value": "Retrieval-augmented generation"
            }
          ],
          "url": "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&explaintext=true&redirects=1&format=json&titles=Retrieval-augmented%20generation"
        },
        "response": {
          "bodySize": 12628,
          "content": {
            "mimeType": "application/json; charset=utf-8",
            "size": 12628,
            "text": "{\"batchcomplete\":\"\",\"query\":{\"pages\":{\"75229858\":{\"pageid\":75229858,\"ns\":0,\"title\":\"Retrieval-augmented generation\",\"extract\":\"Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \\\"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\\\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper from Meta.\\n\\n\\n== RAG and LLM Limitations ==\\nLLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool \\\"Google Bard\\\", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the company\\u2019s stock value. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, \\\"The United States has had one Muslim president, Barack Hussein Obama.\\\" The model retrieved this from an academic book rhetorically titled Barack Hussein Obama: America\\u2019s First Muslim President? The LLM did not \\\"know\\\" or \\\"understand\\\" the context of the title, generating a false statement.\\nLLMs with RAG are programmed to prioritize new information. This technique has been called \\\"prompt stuffing.\\\" Without prompt stuffing, the LLM's input is generated by a user; with prompt stuffing, additional relevant context is added to this input to guide the model\\u2019s response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.\\n\\n\\n== Process ==\\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. AWS states, \\\"RAG allows LLMs to retrieve relevant information from external data sources to generate more accurate and contextually relevant responses\\\" (\\\"indexing\\\"). This approach reduces reliance on static datasets, which can quickly become outdated. When a user submits a query, RAG uses a document retriever to search for relevant content from available sources before incorporating the retrieved information into the model's response (\\\"retrieval\\\"). Ars Technica notes that \\\"when new information becomes available, rather than having to retrain the model, all that\\u2019s needed is to augment the model\\u2019s external knowledge base with the updated information\\\" (\\\"augmentation\\\"). By dynamically integrating relevant data, RAG enables LLMs to generate more informed and contextually grounded responses (\\\"generation\\\"). IBM states that \\\"in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant.\\n\\n\\n=== RAG key stages ===\\n\\n\\n==== Indexing ====\\nTypically, the data to be referenced is converted into LLM embeddings, numerical representations in the form of a large vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs). These embeddings are then stored in a vector database to allow for document retrieval.\\n\\n\\n==== Retrieval ====\\nGiven a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query. This comparison can be done using a variety of methods, which depend in part on the type of indexing used.\\n\\n\\n==== Augmentation ====\\nThe model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query. Newer implementations (as of 2023) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains and using memory and self-improvement to learn from previous retrievals.\\n\\n\\n==== Generation ====\\nFinally, the LLM can generate output based on both the query and the retrieved documents. Some models incorporate extra steps to improve output, such as the re-ranking of retrieved information, context selection, and fine-tuning.\\n\\n\\n== Improvements ==\\n\\nImprovements to the basic process above can be applied at different stages in the RAG flow. \\n\\n\\n=== Encoder ===\\nThese methods focus on the encoding of text as either dense or sparse vectors. Sparse vectors, which encode the identity of a word, are typically dictionary-length and contain mostly zeros. Dense vectors, which encode meaning, are more compact and contain fewer zeros. Various enhancements can improve the way similarities are calculated in the vector stores (databases).  \\n\\nPerformance improves by optimizing how vector similarities are calculated. Dot products enhance similarity scoring, while approximate nearest neighbor (ANN) searches improve retrieval efficiency over K-nearest neighbors (KNN) searches.\\nAccuracy may be improved with Late Interactions, which allow the system to compare words more precisely after retrieval. This helps refine document ranking and improve search relevance.\\nHybrid vector approaches may be used to combine dense vector representations with sparse one-hot vectors, taking advantage of the computational efficiency of sparse dot products over dense vector operations.\\nOther retrieval techniques focus on improving accuracy by refining how documents are selected. Some retrieval methods combine sparse representations, such as SPLADE, with query expansion strategies to improve search accuracy and recall.\\n\\n\\n=== Retriever-centric methods ===\\nThese methods aim to enhance the quality of document retrieval in vector databases:\\n\\nPre-training the retriever using the Inverse Cloze Task (ICT), a technique that helps the model learn retrieval patterns by predicting masked text within documents.\\nProgressive data augmentation, as used in Diverse Augmentation for Generalizable Dense Retrieval (DRAGON), improves dense retrieval by sampling difficult negative examples during training.\\nSupervised retriever optimization aligns retrieval probabilities with the generator model\\u2019s likelihood distribution. This involves retrieving the top-k vectors for a given prompt, scoring the generated response\\u2019s perplexity, and minimizing KL divergence between the retriever\\u2019s selections and the model\\u2019s likelihoods to refine retrieval.\\nReranking techniques can refine retriever performance by prioritizing the most relevant retrieved documents during training.\\n\\n\\n=== Language model ===\\n\\nBy redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts. Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided. The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics. The redesigned language model is shown here.  \\nIt has been reported that Retro is not reproducible, so modifications were made to make it so.  The more reproducible version is called Retro++ and includes in-context RAG.\\n\\n\\n=== Chunking ===\\n\\nChunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.\\n\\nThree types of chunking strategies are:\\n\\nFixed length with overlap. This is fast and easy. Overlapping consecutive chunks helps to maintain semantic context across chunks.\\nSyntax-based chunks can break the document up into sentences. Libraries such as spaCy or NLTK can also help.\\nFile format-based chunking. Certain file types have natural chunks built in, and it's best to respect them. For example, code files are best chunked and vectorized as whole functions or classes. HTML files should leave <table> or base64 encoded <img> elements intact. Similar considerations should be taken for pdf files. Libraries such as Unstructured or Langchain can assist with this method.\\n\\n\\n=== Knowledge graphs ===\\nRather than using documents as a source to vectorize and retrieve from, Knowledge Graphs can be used. One can start with a set of documents, books, or other bodies of text, and convert them to a knowledge graph using one of many methods, including language models. Once the knowledge graph is created, subgraphs can be vectorized, stored in a vector database, and used for retrieval as in plain RAG. The advantage here is that graphs has more recognizable structure than strings of text and this structure can help retrieve more relevant facts for generation. Sometimes this approach is called GraphRAG.\\n\\n\\n=== Hybrid search ===\\nSometimes vector database searches can miss key facts needed to answer a user's question. One way to mitigate this is to do a traditional text search, add those results to the text chunks linked to the retrieved vectors from the vector search, and feed the combined hybrid text into the language model for generation.\\n\\n\\n=== Evaluation and Benchmarks ===\\nRAG systems are commonly evaluated using benchmarks designed to test both retrieval accuracy and generative quality. Popular datasets include BEIR, a suite of information retrieval tasks across diverse domains, and Natural Questions or Google QA for open-domain QA.\\nIn high-stakes domains like law and healthcare, domain-specific benchmarks are increasingly used. For instance, LegalBench-RAG is an open-source benchmark designed to test retrieval quality over legal documents. It evaluates recall and precision for different RAG pipelines using real-world legal questions and documents.\\n\\n\\n== Challenges ==\\nRAG is not a complete solution to the problem of hallucinations in LLMs. According to Ars Technica, \\\"It is not a direct solution because the LLM can still hallucinate around the source material in its response.\\\"\\nWhile RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate answers even when they should indicate uncertainty. According to IBM, this issue can arise when the model lacks the ability to assess its own knowledge limitations.\\nRAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with conflicting information RAG models may struggle to determine which source is accurate. The worst case outcome of this limitation is that the model may combine details from multiple sources producing responses that merge outdated and updated information in a misleading manner. According to the MIT Technology Review, these issues occur because RAG systems may misinterpret the data they retrieve.\\n\\n\\n== References ==\"}}}}"
          },
          "cookies": [
            {
              "domain": ".wikipedia.org",
              "expires": "2026-08-17T00:00:00.000Z",
              "httpOnly": true,
              "name": "WMF-Uniq",
              "path": "/",
              "sameSite": "None",
              "secure": true,
              "value": "48dKkr9iTyCoBjJCKMINcAJSAAAAAFvdkTp0nYrwpo1UEZyQW07hsj7lhbMWlZ8B"
            }
          ],
          "headers": [
            {
              "name": "age",
              "value": "0"
            },
            {
              "name": "cache-control",
              "value": "private, must-revalidate, max-age=0"
            },
            {
              "name": "content-disposition",
              "value": "inline; filename=api-result.json"
            },
            {
              "name": "content-encoding",
              "value": "gzip"
            },
            {
              "name": "content-type",
              "value": "application/json; charset=utf-8"
            },
            {
              "name": "date",
              "value": "Sun, 17 Aug 2025 13:58:26 GMT"
            },
            {
              "name": "nel",
              "value": "{ \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}"
            },
            {
              "name": "report-to",
              "value": "{ \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }"
            },
            {
              "name": "server",
              "value": "mw-api-ext.eqiad.main-8769686bf-pk5mj"
            },
            {
              "name": "server-timing",
              "value": "cache;desc=\"pass\", host;desc=\"cp3067\""
            },
            {
              "name": "set-cookie",
              "value": "WMF-Uniq=48dKkr9iTyCoBjJCKMINcAJSAAAAAFvdkTp0nYrwpo1UEZyQW07hsj7lhbMWlZ8B;Domain=.wikipedia.org;Path=/;HttpOnly;secure;SameSite=None;Expires=Mon, 17 Aug 2026 00:00:00 GMT"
            },
            {
              "name": "strict-transport-security",
              "value": "max-age=106384710; includeSubDomains; preload"
            },
            {
              "name": "transfer-encoding",
              "value": "chunked"
            },
            {
              "name": "vary",
              "value": "Accept-Encoding,X-Subdomain,Treat-as-Untrusted,X-Forwarded-Proto,Cookie,Authorization"
            },
            {
              "name": "x-analytics",
              "value": ""
            },
            {
              "name": "x-cache",
              "value": "cp3067 miss, cp3067 pass"
            },
            {
              "name": "x-cache-status",
              "value": "pass"
            },
            {
              "name": "x-client-ip",
              "value": "31.154.188.106"
            },
            {
              "name": "x-content-type-options",
              "value": "nosniff"
            },
            {
              "name": "x-frame-options",
              "value": "DENY"
            }
          ],
          "headersSize": 1179,
          "httpVersion": "HTTP/1.1",
          "redirectURL": "",
          "status": 200,
          "statusText": "OK"
        },
        "startedDateTime": "2025-08-17T13:58:26.304Z",
        "time": 213,
        "timings": {
          "blocked": -1,
          "connect": -1,
          "dns": -1,
          "receive": 0,
          "send": 0,
          "ssl": -1,
          "wait": 213
        }
      }
    ],
    "pages": [],
    "version": "1.2"
  }
}
