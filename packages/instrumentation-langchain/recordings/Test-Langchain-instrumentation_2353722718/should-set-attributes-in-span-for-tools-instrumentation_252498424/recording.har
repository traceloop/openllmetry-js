{
  "log": {
    "_recordingName": "Test Langchain instrumentation/should set attributes in span for tools instrumentation",
    "creator": {
      "comment": "persister:fs",
      "name": "Polly.JS",
      "version": "6.0.6"
    },
    "entries": [
      {
        "_id": "926cd215cc39afa1cae6237de5d30ce8",
        "_order": 0,
        "cache": {},
        "request": {
          "bodySize": 0,
          "cookies": [],
          "headers": [],
          "headersSize": 109,
          "httpVersion": "HTTP/1.1",
          "method": "GET",
          "queryString": [
            {
              "name": "action",
              "value": "query"
            },
            {
              "name": "list",
              "value": "search"
            },
            {
              "name": "srsearch",
              "value": "Langchain"
            },
            {
              "name": "format",
              "value": "json"
            }
          ],
          "url": "https://en.wikipedia.org/w/api.php?action=query&list=search&srsearch=Langchain&format=json"
        },
        "response": {
          "bodySize": 3068,
          "content": {
            "mimeType": "application/json; charset=utf-8",
            "size": 3068,
            "text": "{\"batchcomplete\":\"\",\"query\":{\"searchinfo\":{\"totalhits\":9,\"suggestion\":\"lang chain\",\"suggestionsnippet\":\"lang chain\"},\"search\":[{\"ns\":0,\"title\":\"LangChain\",\"pageid\":73577626,\"size\":18587,\"wordcount\":748,\"snippet\":\"Free and open-source software portal <span class=\\\"searchmatch\\\">LangChain</span> is a software framework that helps facilitate the integration of large language models (LLMs) into applications\",\"timestamp\":\"2025-06-22T02:48:58Z\"},{\"ns\":0,\"title\":\"Model Context Protocol\",\"pageid\":79706999,\"size\":18473,\"wordcount\":1630,\"snippet\":\"computers or programsPages displaying short descriptions of redirect targets <span class=\\\"searchmatch\\\">LangChain</span>\\u00a0\\u2013 Language model application development framework Machine learning\\u00a0\\u2013\",\"timestamp\":\"2025-07-09T19:03:57Z\"},{\"ns\":0,\"title\":\"Retrieval-augmented generation\",\"pageid\":75229858,\"size\":24381,\"wordcount\":2549,\"snippet\":\"considerations should be taken for pdf files. Libraries such as Unstructured or <span class=\\\"searchmatch\\\">Langchain</span> can assist with this method. Rather than using documents as a source to\",\"timestamp\":\"2025-07-16T15:32:33Z\"},{\"ns\":0,\"title\":\"Large language model\",\"pageid\":73248112,\"size\":136690,\"wordcount\":14140,\"snippet\":\" GitHub. &quot;Core Concepts: Long-term Memory in LLM Applications&quot;. <span class=\\\"searchmatch\\\">langchain</span>-ai.github.io. Wang, Lei; Ma, Chen; Feng, Xueyang; Zhang, Zeyu; Yang, Hao;\",\"timestamp\":\"2025-07-20T06:38:48Z\"},{\"ns\":0,\"title\":\"Milvus (vector database)\",\"pageid\":77949393,\"size\":12723,\"wordcount\":1002,\"snippet\":\"monitoring and alerts, as well as generative AI frameworks Haystack, <span class=\\\"searchmatch\\\">LangChain</span>, IBM Watsonx, and those provided by OpenAI. Several storage providers\",\"timestamp\":\"2025-07-19T11:50:17Z\"},{\"ns\":0,\"title\":\"FAISS\",\"pageid\":78493979,\"size\":13883,\"wordcount\":1163,\"snippet\":\"similarity search benchmarks. FAISS has an integration with Haystack, <span class=\\\"searchmatch\\\">LangChain</span> frameworks. Various advanced code snippets for FAISS can be found on its\",\"timestamp\":\"2025-07-12T00:04:39Z\"},{\"ns\":0,\"title\":\"Intelligent agent\",\"pageid\":2711317,\"size\":74099,\"wordcount\":6899,\"snippet\":\"Rumination, and Coze (by ByteDance). Frameworks for building AI agents include <span class=\\\"searchmatch\\\">LangChain</span>, as well as tools such as CAMEL, Microsoft AutoGen, and OpenAI Swarm.\",\"timestamp\":\"2025-07-15T14:59:47Z\"},{\"ns\":0,\"title\":\"Sentence embedding\",\"pageid\":58348103,\"size\":9223,\"wordcount\":973,\"snippet\":\"knowledge bases through the usage of vector indexing for semantic search. <span class=\\\"searchmatch\\\">LangChain</span> for instance utilizes sentence transformers for purposes of indexing documents\",\"timestamp\":\"2025-01-10T19:07:44Z\"},{\"ns\":0,\"title\":\"DataStax\",\"pageid\":37892981,\"size\":22066,\"wordcount\":1861,\"snippet\":\"commercial offering for RAG (retrieval-augmented generation) based on <span class=\\\"searchmatch\\\">LangChain</span> and Astra DB vector search. On February 25, 2025, IBM announced its intention\",\"timestamp\":\"2025-06-23T08:19:15Z\"}]}}"
          },
          "cookies": [
            {
              "domain": ".wikipedia.org",
              "expires": "2026-07-20T00:00:00.000Z",
              "httpOnly": true,
              "name": "WMF-Uniq",
              "path": "/",
              "sameSite": "None",
              "secure": true,
              "value": "ojqeHgvJ5t1VqQGJklIr3AI2AAAAAFvdxZs2IFdh3Dh1p87Lh2YhyDxUUF-5FMLe"
            }
          ],
          "headers": [
            {
              "name": "age",
              "value": "0"
            },
            {
              "name": "cache-control",
              "value": "private, must-revalidate, max-age=0"
            },
            {
              "name": "content-disposition",
              "value": "inline; filename=api-result.json"
            },
            {
              "name": "content-encoding",
              "value": "gzip"
            },
            {
              "name": "content-type",
              "value": "application/json; charset=utf-8"
            },
            {
              "name": "date",
              "value": "Sun, 20 Jul 2025 13:53:24 GMT"
            },
            {
              "name": "nel",
              "value": "{ \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}"
            },
            {
              "name": "report-to",
              "value": "{ \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }"
            },
            {
              "name": "server",
              "value": "mw-api-ext.eqiad.main-689bdc95f7-jbhk7"
            },
            {
              "name": "server-timing",
              "value": "cache;desc=\"pass\", host;desc=\"cp3067\""
            },
            {
              "name": "set-cookie",
              "value": "WMF-Uniq=ojqeHgvJ5t1VqQGJklIr3AI2AAAAAFvdxZs2IFdh3Dh1p87Lh2YhyDxUUF-5FMLe;Domain=.wikipedia.org;Path=/;HttpOnly;secure;SameSite=None;Expires=Mon, 20 Jul 2026 00:00:00 GMT"
            },
            {
              "name": "strict-transport-security",
              "value": "max-age=106384710; includeSubDomains; preload"
            },
            {
              "name": "transfer-encoding",
              "value": "chunked"
            },
            {
              "name": "vary",
              "value": "Accept-Encoding,X-Subdomain,Treat-as-Untrusted,X-Forwarded-Proto,Cookie,Authorization"
            },
            {
              "name": "x-cache",
              "value": "cp3067 miss, cp3067 pass"
            },
            {
              "name": "x-cache-status",
              "value": "pass"
            },
            {
              "name": "x-client-ip",
              "value": "31.154.188.106"
            },
            {
              "name": "x-content-type-options",
              "value": "nosniff"
            },
            {
              "name": "x-frame-options",
              "value": "DENY"
            },
            {
              "name": "x-search-id",
              "value": "8kmhmustvxs6c4s6f4vvu9ey7"
            }
          ],
          "headersSize": 1205,
          "httpVersion": "HTTP/1.1",
          "redirectURL": "",
          "status": 200,
          "statusText": "OK"
        },
        "startedDateTime": "2025-07-20T13:53:23.876Z",
        "time": 441,
        "timings": {
          "blocked": -1,
          "connect": -1,
          "dns": -1,
          "receive": 0,
          "send": 0,
          "ssl": -1,
          "wait": 441
        }
      },
      {
        "_id": "926cd215cc39afa1cae6237de5d30ce8",
        "_order": 1,
        "cache": {},
        "request": {
          "bodySize": 0,
          "cookies": [],
          "headers": [],
          "headersSize": 138,
          "httpVersion": "HTTP/1.1",
          "method": "GET",
          "queryString": [
            {
              "name": "action",
              "value": "query"
            },
            {
              "name": "prop",
              "value": "extracts"
            },
            {
              "name": "explaintext",
              "value": "true"
            },
            {
              "name": "redirects",
              "value": "1"
            },
            {
              "name": "format",
              "value": "json"
            },
            {
              "name": "titles",
              "value": "LangChain"
            }
          ],
          "url": "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&explaintext=true&redirects=1&format=json&titles=LangChain"
        },
        "response": {
          "bodySize": 3206,
          "content": {
            "mimeType": "application/json; charset=utf-8",
            "size": 3206,
            "text": "{\"batchcomplete\":\"\",\"query\":{\"pages\":{\"73577626\":{\"pageid\":73577626,\"ns\":0,\"title\":\"LangChain\",\"extract\":\"LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n== History ==\\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project's Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.\\nIn the third quarter of 2023, the LangChain Expression Language (LCEL) was introduced, which provides a declarative way to define chains of actions.\\nIn October 2023 LangChain introduced LangServe, a deployment tool to host LCEL code as a production-ready API.\\n\\n\\n== Capabilities ==\\nLangChain's developers highlight the framework's applicability to use-cases including chatbots, retrieval-augmented generation,  document summarization, and synthetic data generation.\\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \\\"todo\\\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database to store and retrieve vector embeddings; Weaviate vector database to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\\n\\n\\n== LangChain tools ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nOfficial website\\nDiscord server support hub\\nLangchain-ai on GitHub\"}}}}"
          },
          "cookies": [
            {
              "domain": ".wikipedia.org",
              "expires": "2026-07-20T00:00:00.000Z",
              "httpOnly": true,
              "name": "WMF-Uniq",
              "path": "/",
              "sameSite": "None",
              "secure": true,
              "value": "gNVrpHi22pw6_Z_m1c2naQI2AAAAAFvdnSFwoyjuuyNpeE6_Jdpe2JYHMBaAKVnD"
            }
          ],
          "headers": [
            {
              "name": "age",
              "value": "0"
            },
            {
              "name": "cache-control",
              "value": "private, must-revalidate, max-age=0"
            },
            {
              "name": "content-disposition",
              "value": "inline; filename=api-result.json"
            },
            {
              "name": "content-encoding",
              "value": "gzip"
            },
            {
              "name": "content-type",
              "value": "application/json; charset=utf-8"
            },
            {
              "name": "date",
              "value": "Sun, 20 Jul 2025 13:53:24 GMT"
            },
            {
              "name": "nel",
              "value": "{ \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}"
            },
            {
              "name": "report-to",
              "value": "{ \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }"
            },
            {
              "name": "server",
              "value": "mw-api-ext.eqiad.main-689bdc95f7-qg2vm"
            },
            {
              "name": "server-timing",
              "value": "cache;desc=\"pass\", host;desc=\"cp3067\""
            },
            {
              "name": "set-cookie",
              "value": "WMF-Uniq=gNVrpHi22pw6_Z_m1c2naQI2AAAAAFvdnSFwoyjuuyNpeE6_Jdpe2JYHMBaAKVnD;Domain=.wikipedia.org;Path=/;HttpOnly;secure;SameSite=None;Expires=Mon, 20 Jul 2026 00:00:00 GMT"
            },
            {
              "name": "strict-transport-security",
              "value": "max-age=106384710; includeSubDomains; preload"
            },
            {
              "name": "transfer-encoding",
              "value": "chunked"
            },
            {
              "name": "vary",
              "value": "Accept-Encoding,X-Subdomain,Treat-as-Untrusted,X-Forwarded-Proto,Cookie,Authorization"
            },
            {
              "name": "x-cache",
              "value": "cp3067 miss, cp3067 pass"
            },
            {
              "name": "x-cache-status",
              "value": "pass"
            },
            {
              "name": "x-client-ip",
              "value": "31.154.188.106"
            },
            {
              "name": "x-content-type-options",
              "value": "nosniff"
            },
            {
              "name": "x-frame-options",
              "value": "DENY"
            }
          ],
          "headersSize": 1165,
          "httpVersion": "HTTP/1.1",
          "redirectURL": "",
          "status": 200,
          "statusText": "OK"
        },
        "startedDateTime": "2025-07-20T13:53:24.320Z",
        "time": 194,
        "timings": {
          "blocked": -1,
          "connect": -1,
          "dns": -1,
          "receive": 0,
          "send": 0,
          "ssl": -1,
          "wait": 194
        }
      },
      {
        "_id": "926cd215cc39afa1cae6237de5d30ce8",
        "_order": 2,
        "cache": {},
        "request": {
          "bodySize": 0,
          "cookies": [],
          "headers": [],
          "headersSize": 155,
          "httpVersion": "HTTP/1.1",
          "method": "GET",
          "queryString": [
            {
              "name": "action",
              "value": "query"
            },
            {
              "name": "prop",
              "value": "extracts"
            },
            {
              "name": "explaintext",
              "value": "true"
            },
            {
              "name": "redirects",
              "value": "1"
            },
            {
              "name": "format",
              "value": "json"
            },
            {
              "name": "titles",
              "value": "Model Context Protocol"
            }
          ],
          "url": "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&explaintext=true&redirects=1&format=json&titles=Model%20Context%20Protocol"
        },
        "response": {
          "bodySize": 8572,
          "content": {
            "mimeType": "application/json; charset=utf-8",
            "size": 8572,
            "text": "{\"batchcomplete\":\"\",\"query\":{\"pages\":{\"79706999\":{\"pageid\":79706999,\"ns\":0,\"title\":\"Model Context Protocol\",\"extract\":\"The Model Context Protocol (MCP) is an open standard, open-source framework introduced by Anthropic in November 2024 to standardize the way artificial intelligence (AI) systems like large language models (LLMs) integrate and share data with external tools, systems, and data sources. MCP provides a universal interface for reading files, executing functions, and handling contextual prompts. Following its announcement, the protocol was adopted by major AI providers, including OpenAI and Google DeepMind.\\n\\n\\n== Background ==\\nThe protocol was announced by Anthropic in November 2024 as an open standard for connecting AI assistants to data systems such as content repositories, business management tools, and development environments. It aims to address the challenge of information silos and legacy systems. Before MCP, developers often had to build custom connectors for each data source or tool, resulting in what Anthropic described as an \\\"N\\u00d7M\\\" data integration problem.\\nEarlier stop-gap approaches - such as OpenAI\\u2019s 2023 \\u201cfunction-calling\\u201d API and the ChatGPT plug-in framework - solved similar problems but required vendor-specific connectors. MCP\\u2019s authors note that the protocol deliberately re-uses the message-flow ideas of the Language Server Protocol (LSP) and is transported over JSON-RPC 2.0.\\n\\n\\n== Features ==\\nMCP defines a standardized framework for integrating AI systems with external data sources and tools. It includes specifications for data ingestion and transformation, contextual metadata tagging, and AI interoperability across different platforms. The protocol also supports secure, bidirectional connections between data sources and AI-powered tools.\\nMCP enables developers to expose their data via MCP servers or to develop AI applications\\u2014referred to as MCP clients\\u2014that connect to these servers. Key components of the protocol include a formal protocol specification and software development kits (SDKs), local MCP server support in Claude Desktop applications, and an open-source repository of MCP server implementations.\\n\\n\\n== Applications ==\\nMCP has been applied in domains such as software development, business process automation, and natural language automation.\\nOne prominent use case is in desktop assistants, where applications such as the Claude Desktop app deploy local MCP servers to enable secure access to system tools and user files. In enterprise settings, internal assistants are enhanced with MCP to retrieve data from proprietary documents, CRM systems, and internal knowledge bases\\u2014companies like Block have integrated MCP into their internal tooling for this purpose.\\nMCP also plays a critical role in multi-tool agent workflows, allowing agentic AI systems to coordinate multiple tools\\u2014for example, combining document lookup with messaging APIs\\u2014to support advanced, chain-of-thought reasoning across distributed resources.\\nIn the field of natural language data access, MCP enables applications such as AI2SQL to bridge language models with structured databases, allowing plain-language queries.\\nMCP has been adopted for academic research workflows through integrations with reference management systems like Zotero. Multiple server implementations allow researchers to perform semantic searches across their libraries, extract PDF annotations, and generate literature reviews through AI-assisted analysis.\\nThe protocol has become increasingly common in software development tools. Integrated development environments (IDEs) like Zed, coding platforms such as Replit, and code intelligence tools like Sourcegraph have adopted MCP to grant AI coding assistants real-time access to project context. This integration is especially valuable for workflows like \\\"vibe coding\\\", where continuous, adaptive assistance is essential.\\nIn web application development, companies like Wix have embedded MCP servers into their platforms. This allows AI tools to interact with live website data, enabling dynamic content generation and on-the-fly edits. Such capabilities are central to Wix\\u2019s AI-driven development tools.\\n\\n\\n== Implementation ==\\nThe protocol was released with software development kits (SDKs) in programming languages including Python, TypeScript, C# and Java. Anthropic maintains an open-source repository of reference MCP server implementations for popular enterprise systems including Google Drive, Slack, GitHub, Git, Postgres, Puppeteer and Stripe. Developers can create custom MCP servers to connect proprietary systems or specialized data sources to AI systems.\\nThe protocol's open standard allows organizations to build tailored connections while maintaining compatibility with the broader MCP ecosystem. AI systems can then leverage these custom connections to provide domain-specific assistance while respecting data access permissions.\\n\\n\\n== Adoption ==\\nIn March 2025, OpenAI officially adopted the MCP, following a decision to integrate the standard across its products, including the ChatGPT desktop app, OpenAI's Agents SDK, and the Responses API. Sam Altman described the adoption of MCP as a step toward standardizing AI tool connectivity. Prior to OpenAI's adoption, the potential benefits of MCP had been discussed extensively within the developer community, particularly for simplifying development in multi-model environments. \\nBy adopting MCP, OpenAI joins other organizations such as Block, Replit, and Sourcegraph in incorporating the protocol into their platforms. This wide adoption highlights MCP's potential to become a universal open standard for AI system connectivity and interoperability. MCP can be integrated with Microsoft Semantic Kernel, and Azure OpenAI. MCP servers can be deployed to Cloudflare.\\nDemis Hassabis, CEO of Google DeepMind, confirmed in April 2025 MCP support in the upcoming Gemini models and related infrastructure, describing the protocol as \\\"rapidly becoming an open standard for the AI agentic era\\\".\\nMany MCP servers have since been added, allowing integration of LLMs with diverse applications.\\n\\n\\n== Reception ==\\nThe Verge reported that MCP addresses a growing demand for AI agents that are contextually aware and capable of securely pulling from diverse sources. The protocol's rapid uptake by OpenAI, Google DeepMind, and toolmakers like Zed and Sourcegraph suggests growing consensus around its utility.\\nIn April 2025, security researchers released analysis that there are multiple outstanding security issues with MCP, including prompt injection, tool permissions where combining tools can exfiltrate files, and lookalike tools can silently replace trusted ones.\\nIt has been likened to OpenAPI, a similar specification that aims to describe APIs.\\n\\n\\n== See also ==\\nAI governance \\u2013 Guidelines and laws to regulate AIPages displaying short descriptions of redirect targets\\nApplication programming interface \\u2013 Connection between computers or programsPages displaying short descriptions of redirect targets\\nLangChain \\u2013 Language model application development framework\\nMachine learning \\u2013 Study of algorithms that improve automatically through experience\\nSoftware agent \\u2013 Computer program acting for a user\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nHou, Xinyi; Zhao, Yanjie; Wang, Shenao; Wang, Haoyu (2025). \\\"Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions\\\". arXiv:2503.23278 [cs.CR].\\nEdwards, Benj (April 1, 2025). \\\"MCP: The new \\\"USB-C for AI\\\" that's bringing fierce rivals together\\\". Ars Technica.\\nJackson, Fiona (March 28, 2025). \\\"OpenAI Agents Now Support Rival Anthropic's Protocol, Making Data Access 'Simpler, More Reliable'\\\". TechRepublic.\\nMasson, Colin (March 25, 2025). \\\"Context Is the Missing Link: The Emergence of the Model Context Protocol in Industrial AI\\\". ARC Advisory Group.\\nJimin Kim; Anita Lewis; Justin Lewis; Laith Al-Saadoon; Paul Vincent; Pranjali Bhandari (April 1, 2025). \\\"Introducing AWS MCP Servers for code assistants (Part 1)\\\". Amazon AWS.\\nDesai, Zankar (March 19, 2025). \\\"Introducing Model Context Protocol (MCP) in Copilot Studio: Simplified Integration with AI Apps and Agents\\\". Microsoft Copilot Studio Blog, Microsoft.\\nWagner, Tim (May 13, 2025). \\\"Understanding Model Context Protocol (MCP)\\\". Vendia.\\n\\n\\n== External links ==\\nOfficial website\\nmodelcontextprotocol on GitHub\\nSDK documentation from OpenAI\"}}}}"
          },
          "cookies": [
            {
              "domain": ".wikipedia.org",
              "expires": "2026-07-20T00:00:00.000Z",
              "httpOnly": true,
              "name": "WMF-Uniq",
              "path": "/",
              "sameSite": "None",
              "secure": true,
              "value": "LANP48slxzUfYZgx7ZTylQI2AAAAAFvdlRNyWBZWRKnZ3WEj_cr559s6EMhf7rP0"
            }
          ],
          "headers": [
            {
              "name": "age",
              "value": "0"
            },
            {
              "name": "cache-control",
              "value": "private, must-revalidate, max-age=0"
            },
            {
              "name": "content-disposition",
              "value": "inline; filename=api-result.json"
            },
            {
              "name": "content-encoding",
              "value": "gzip"
            },
            {
              "name": "content-type",
              "value": "application/json; charset=utf-8"
            },
            {
              "name": "date",
              "value": "Sun, 20 Jul 2025 13:53:24 GMT"
            },
            {
              "name": "nel",
              "value": "{ \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}"
            },
            {
              "name": "report-to",
              "value": "{ \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }"
            },
            {
              "name": "server",
              "value": "mw-api-ext.eqiad.main-689bdc95f7-sdrgb"
            },
            {
              "name": "server-timing",
              "value": "cache;desc=\"pass\", host;desc=\"cp3067\""
            },
            {
              "name": "set-cookie",
              "value": "WMF-Uniq=LANP48slxzUfYZgx7ZTylQI2AAAAAFvdlRNyWBZWRKnZ3WEj_cr559s6EMhf7rP0;Domain=.wikipedia.org;Path=/;HttpOnly;secure;SameSite=None;Expires=Mon, 20 Jul 2026 00:00:00 GMT"
            },
            {
              "name": "strict-transport-security",
              "value": "max-age=106384710; includeSubDomains; preload"
            },
            {
              "name": "transfer-encoding",
              "value": "chunked"
            },
            {
              "name": "vary",
              "value": "Accept-Encoding,X-Subdomain,Treat-as-Untrusted,X-Forwarded-Proto,Cookie,Authorization"
            },
            {
              "name": "x-cache",
              "value": "cp3067 miss, cp3067 pass"
            },
            {
              "name": "x-cache-status",
              "value": "pass"
            },
            {
              "name": "x-client-ip",
              "value": "31.154.188.106"
            },
            {
              "name": "x-content-type-options",
              "value": "nosniff"
            },
            {
              "name": "x-frame-options",
              "value": "DENY"
            }
          ],
          "headersSize": 1165,
          "httpVersion": "HTTP/1.1",
          "redirectURL": "",
          "status": 200,
          "statusText": "OK"
        },
        "startedDateTime": "2025-07-20T13:53:24.516Z",
        "time": 196,
        "timings": {
          "blocked": -1,
          "connect": -1,
          "dns": -1,
          "receive": 0,
          "send": 0,
          "ssl": -1,
          "wait": 196
        }
      },
      {
        "_id": "926cd215cc39afa1cae6237de5d30ce8",
        "_order": 3,
        "cache": {},
        "request": {
          "bodySize": 0,
          "cookies": [],
          "headers": [],
          "headersSize": 161,
          "httpVersion": "HTTP/1.1",
          "method": "GET",
          "queryString": [
            {
              "name": "action",
              "value": "query"
            },
            {
              "name": "prop",
              "value": "extracts"
            },
            {
              "name": "explaintext",
              "value": "true"
            },
            {
              "name": "redirects",
              "value": "1"
            },
            {
              "name": "format",
              "value": "json"
            },
            {
              "name": "titles",
              "value": "Retrieval-augmented generation"
            }
          ],
          "url": "https://en.wikipedia.org/w/api.php?action=query&prop=extracts&explaintext=true&redirects=1&format=json&titles=Retrieval-augmented%20generation"
        },
        "response": {
          "bodySize": 13138,
          "content": {
            "mimeType": "application/json; charset=utf-8",
            "size": 13138,
            "text": "{\"batchcomplete\":\"\",\"query\":{\"pages\":{\"75229858\":{\"pageid\":75229858,\"ns\":0,\"title\":\"Retrieval-augmented generation\",\"extract\":\"Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \\\"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\\\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper from Meta.\\n\\n\\n== RAG and LLM Limitations ==\\nLLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool \\\"Google Bard\\\", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the company\\u2019s stock value. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, \\\"The United States has had one Muslim president, Barack Hussein Obama.\\\" The model retrieved this from an academic book rhetorically titled Barack Hussein Obama: America\\u2019s First Muslim President? The LLM did not \\\"know\\\" or \\\"understand\\\" the context of the title, generating a false statement.\\nLLMs with RAG are programmed to prioritize new information. This technique has been called \\\"prompt stuffing.\\\" Without prompt stuffing, the LLM's input is generated by a user; with prompt stuffing, additional relevant context is added to this input to guide the model\\u2019s response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.\\n\\n\\n== Process ==\\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. AWS states, \\\"RAG allows LLMs to retrieve relevant information from external data sources to generate more accurate and contextually relevant responses\\\" (\\\"indexing\\\"). This approach reduces reliance on static datasets, which can quickly become outdated. When a user submits a query, RAG uses a document retriever to search for relevant content from available sources before incorporating the retrieved information into the model's response (\\\"retrieval\\\"). Ars Technica notes that \\\"when new information becomes available, rather than having to retrain the model, all that\\u2019s needed is to augment the model\\u2019s external knowledge base with the updated information\\\" (\\\"augmentation\\\"). By dynamically integrating relevant data, RAG enables LLMs to generate more informed and contextually grounded responses (\\\"generation\\\"). IBM states that \\\"in the generative phase, the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant.\\n\\n\\n=== RAG key stages ===\\n\\n\\n==== Indexing ====\\nTypically, the data to be referenced is converted into LLM embeddings, numerical representations in the form of a large vector space. RAG can be used on unstructured (usually text), semi-structured, or structured data (for example knowledge graphs). These embeddings are then stored in a vector database to allow for document retrieval.\\n\\n\\n==== Retrieval ====\\nGiven a user query, a document retriever is first called to select the most relevant documents that will be used to augment the query. This comparison can be done using a variety of methods, which depend in part on the type of indexing used.\\n\\n\\n==== Augmentation ====\\nThe model feeds this relevant retrieved information into the LLM via prompt engineering of the user's original query. Newer implementations (as of 2023) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains and using memory and self-improvement to learn from previous retrievals.\\n\\n\\n==== Generation ====\\nFinally, the LLM can generate output based on both the query and the retrieved documents. Some models incorporate extra steps to improve output, such as the re-ranking of retrieved information, context selection, and fine-tuning.\\n\\n\\n== Improvements ==\\n\\nImprovements to the basic process above can be applied at different stages in the RAG flow. \\n\\n\\n=== Encoder ===\\nThese methods focus on the encoding of text as either dense or sparse vectors. Sparse vectors, which encode the identity of a word, are typically dictionary-length and contain mostly zeros. Dense vectors, which encode meaning, are more compact and contain fewer zeros. Various enhancements can improve the way similarities are calculated in the vector stores (databases).  \\n\\nPerformance improves by optimizing how vector similarities are calculated. Dot products enhance similarity scoring, while approximate nearest neighbor (ANN) searches improve retrieval efficiency over K-nearest neighbors (KNN) searches.\\nAccuracy may be improved with Late Interactions, which allow the system to compare words more precisely after retrieval. This helps refine document ranking and improve search relevance.\\nHybrid vector approaches may be used to combine dense vector representations with sparse one-hot vectors, taking advantage of the computational efficiency of sparse dot products over dense vector operations.\\nOther retrieval techniques focus on improving accuracy by refining how documents are selected. Some retrieval methods combine sparse representations, such as SPLADE, with query expansion strategies to improve search accuracy and recall.\\n\\n\\n=== Retriever-centric methods ===\\nThese methods aim to enhance the quality of document retrieval in vector databases:\\n\\nPre-training the retriever using the Inverse Cloze Task (ICT), a technique that helps the model learn retrieval patterns by predicting masked text within documents.\\nProgressive data augmentation, as used in Diverse Augmentation for Generalizable Dense Retrieval (DRAGON), improves dense retrieval by sampling difficult negative examples during training.\\nSupervised retriever optimization aligns retrieval probabilities with the generator model\\u2019s likelihood distribution. This involves retrieving the top-k vectors for a given prompt, scoring the generated response\\u2019s perplexity, and minimizing KL divergence between the retriever\\u2019s selections and the model\\u2019s likelihoods to refine retrieval.\\nReranking techniques can refine retriever performance by prioritizing the most relevant retrieved documents during training.\\n\\n\\n=== Language model ===\\n\\nBy redesigning the language model with the retriever in mind, a 25-time smaller network can get comparable perplexity as its much larger counterparts. Because it is trained from scratch, this method (Retro) incurs the high cost of training runs that the original RAG scheme avoided. The hypothesis is that by giving domain knowledge during training, Retro needs less focus on the domain and can devote its smaller weight resources only to language semantics. The redesigned language model is shown here.  \\nIt has been reported that Retro is not reproducible, so modifications were made to make it so.  The more reproducible version is called Retro++ and includes in-context RAG.\\n\\n\\n=== Chunking ===\\n\\nChunking involves various strategies for breaking up the data into vectors so the retriever can find details in it.\\n\\nThree types of chunking strategies are:\\n\\nFixed length with overlap. This is fast and easy. Overlapping consecutive chunks helps to maintain semantic context across chunks.\\nSyntax-based chunks can break the document up into sentences. Libraries such as spaCy or NLTK can also help.\\nFile format-based chunking. Certain file types have natural chunks built in, and it's best to respect them. For example, code files are best chunked and vectorized as whole functions or classes. HTML files should leave <table> or base64 encoded <img> elements intact. Similar considerations should be taken for pdf files. Libraries such as Unstructured or Langchain can assist with this method.\\n\\n\\n=== Knowledge graphs ===\\nRather than using documents as a source to vectorize and retrieve from, Knowledge Graphs can be used. One can start with a set of documents, books, or other bodies of text, and convert them to a knowledge graph using one of many methods, including language models. Once the knowledge graph is created, subgraphs can be vectorized, stored in a vector database, and used for retrieval as in plain RAG. The advantage here is that graphs has more recognizable structure than strings of text and this structure can help retrieve more relevant facts for generation. Sometimes this approach is called GraphRAG.\\n\\n\\n=== Hybrid search ===\\nSometimes vector database searches can miss key facts needed to answer a user's question. One way to mitigate this is to do a traditional text search, add those results to the text chunks linked to the retrieved vectors from the vector search, and feed the combined hybrid text into the language model for generation.\\n\\n\\n=== Late-interaction Search ===\\nSince vector search relies on embedding individual chunks, a lot of granular, token-level information cannot be obtained via pure hybrid of vector search. For higher accuracy, one can create embeddings out of individual tokens instead and compute the Chamfer distance between them. This leads to significantly better results at the cost of speed. Modern solutions such as Morphik make this technique scalable by using a combination of software and hardware acceleration. \\n\\n\\n=== Evaluation and Benchmarks ===\\nRAG systems are commonly evaluated using benchmarks designed to test both retrieval accuracy and generative quality. Popular datasets include BEIR, a suite of information retrieval tasks across diverse domains, and Natural Questions or Google QA for open-domain QA.\\nIn high-stakes domains like law and healthcare, domain-specific benchmarks are increasingly used. For instance, LegalBench-RAG is an open-source benchmark designed to test retrieval quality over legal documents. It evaluates recall and precision for different RAG pipelines using real-world legal questions and documents.\\n\\n\\n== Challenges ==\\nRAG is not a complete solution to the problem of hallucinations in LLMs. According to Ars Technica, \\\"It is not a direct solution because the LLM can still hallucinate around the source material in its response.\\\"\\nWhile RAG improves the accuracy of large language models (LLMs), it does not eliminate all challenges. One limitation is that while RAG reduces the need for frequent model retraining, it does not remove it entirely. Additionally, LLMs may struggle to recognize when they lack sufficient information to provide a reliable response. Without specific training, models may generate answers even when they should indicate uncertainty. According to IBM, this issue can arise when the model lacks the ability to assess its own knowledge limitations.\\nRAG systems may retrieve factually correct but misleading sources, leading to errors in interpretation. In some cases, an LLM may extract statements from a source without considering its context, resulting in an incorrect conclusion. Additionally, when faced with conflicting information RAG models may struggle to determine which source is accurate. The worst case outcome of this limitation is that the model may combine details from multiple sources producing responses that merge outdated and updated information in a misleading manner. According to the MIT Technology Review, these issues occur because RAG systems may misinterpret the data they retrieve.\\n\\n\\n== References ==\"}}}}"
          },
          "cookies": [
            {
              "domain": ".wikipedia.org",
              "expires": "2026-07-20T00:00:00.000Z",
              "httpOnly": true,
              "name": "WMF-Uniq",
              "path": "/",
              "sameSite": "None",
              "secure": true,
              "value": "r8c74wDq12Z0KQ-WVCWg7AI2AAAAAFvdi2IxuqV9vywIkzvL4Nwwp73cq64Ten66"
            }
          ],
          "headers": [
            {
              "name": "age",
              "value": "0"
            },
            {
              "name": "cache-control",
              "value": "private, must-revalidate, max-age=0"
            },
            {
              "name": "content-disposition",
              "value": "inline; filename=api-result.json"
            },
            {
              "name": "content-encoding",
              "value": "gzip"
            },
            {
              "name": "content-type",
              "value": "application/json; charset=utf-8"
            },
            {
              "name": "date",
              "value": "Sun, 20 Jul 2025 13:53:24 GMT"
            },
            {
              "name": "nel",
              "value": "{ \"report_to\": \"wm_nel\", \"max_age\": 604800, \"failure_fraction\": 0.05, \"success_fraction\": 0.0}"
            },
            {
              "name": "report-to",
              "value": "{ \"group\": \"wm_nel\", \"max_age\": 604800, \"endpoints\": [{ \"url\": \"https://intake-logging.wikimedia.org/v1/events?stream=w3c.reportingapi.network_error&schema_uri=/w3c/reportingapi/network_error/1.0.0\" }] }"
            },
            {
              "name": "server",
              "value": "mw-api-ext.eqiad.main-689bdc95f7-hswng"
            },
            {
              "name": "server-timing",
              "value": "cache;desc=\"pass\", host;desc=\"cp3067\""
            },
            {
              "name": "set-cookie",
              "value": "WMF-Uniq=r8c74wDq12Z0KQ-WVCWg7AI2AAAAAFvdi2IxuqV9vywIkzvL4Nwwp73cq64Ten66;Domain=.wikipedia.org;Path=/;HttpOnly;secure;SameSite=None;Expires=Mon, 20 Jul 2026 00:00:00 GMT"
            },
            {
              "name": "strict-transport-security",
              "value": "max-age=106384710; includeSubDomains; preload"
            },
            {
              "name": "transfer-encoding",
              "value": "chunked"
            },
            {
              "name": "vary",
              "value": "Accept-Encoding,X-Subdomain,Treat-as-Untrusted,X-Forwarded-Proto,Cookie,Authorization"
            },
            {
              "name": "x-cache",
              "value": "cp3067 miss, cp3067 pass"
            },
            {
              "name": "x-cache-status",
              "value": "pass"
            },
            {
              "name": "x-client-ip",
              "value": "31.154.188.106"
            },
            {
              "name": "x-content-type-options",
              "value": "nosniff"
            },
            {
              "name": "x-frame-options",
              "value": "DENY"
            }
          ],
          "headersSize": 1165,
          "httpVersion": "HTTP/1.1",
          "redirectURL": "",
          "status": 200,
          "statusText": "OK"
        },
        "startedDateTime": "2025-07-20T13:53:24.713Z",
        "time": 198,
        "timings": {
          "blocked": -1,
          "connect": -1,
          "dns": -1,
          "receive": 0,
          "send": 0,
          "ssl": -1,
          "wait": 198
        }
      }
    ],
    "pages": [],
    "version": "1.2"
  }
}
